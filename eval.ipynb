{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC,\n",
    "    Wav2Vec2Processor,\n",
    ")\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead \n",
    "import IPython.display as ipd\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"./xls-r-model-dir/checkpoint-xxxx/\"\n",
    "processor_name = \"./xls-r-model-dir/\"\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ckiplab/gpt2-base-chinese\")  \n",
    "lm_model = AutoModelWithLMHead.from_pretrained(\"ckiplab/gpt2-base-chinese\").to(device)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)\n",
    "processor = Wav2Vec2Processor.from_pretrained(processor_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_to_data(file,sampling_rate=16_000):\n",
    "    batch = {}\n",
    "    speech, _ = torchaudio.load(file)\n",
    "    if sampling_rate != '16_000' or sampling_rate != '16000':\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16_000)\n",
    "        batch[\"speech\"] = resampler.forward(speech.squeeze(0)).numpy()\n",
    "        batch[\"sampling_rate\"] = resampler.new_freq\n",
    "    else:\n",
    "        batch[\"speech\"] = speech.squeeze(0).numpy()\n",
    "        batch[\"sampling_rate\"] = '16000'\n",
    "    return batch\n",
    "\n",
    "def predict_beam(data,beamsize=3):\n",
    "    features = processor(data[\"speech\"], sampling_rate=data[\"sampling_rate\"], padding=True, return_tensors=\"pt\")\n",
    "    input_values = features.input_values.to(device)\n",
    "    attention_mask = features.attention_mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values, attention_mask=attention_mask).logits\n",
    "    decoded_results = []\n",
    "    for logit in logits:\n",
    "        sequences = [[[], 1.0]]\n",
    "        pred_ids = torch.argmax(logit, dim=-1)\n",
    "        mask = pred_ids.ge(1).unsqueeze(-1).expand(logit.size())\n",
    "        vocab_size = logit.size()[-1]\n",
    "        voice_prob = torch.nn.functional.softmax((torch.masked_select(logit, mask).view(-1,vocab_size)),dim=-1)\n",
    "        while True:\n",
    "            all_candidates = list()\n",
    "            exceed = False\n",
    "            for seq in sequences:\n",
    "                tokens, score = seq\n",
    "                gpt_input = torch.tensor([tokenizer.cls_token_id]+tokens).to(device)\n",
    "                gpt_prob = torch.nn.functional.softmax(lm_model(gpt_input).logits, dim=-1)[:len(gpt_input),:]\n",
    "                if len(gpt_input) >= len(voice_prob):\n",
    "                    exceed = True\n",
    "                comb_pred_ids = gpt_prob*voice_prob[:len(gpt_input)]\n",
    "                v,i = torch.topk(comb_pred_ids,50,dim=-1)\n",
    "                for tok_id,tok_prob in zip(i.tolist()[-1],v.tolist()[-1]):\n",
    "                    candidate = [tokens + [tok_id], score + -log(tok_prob)]\n",
    "                    all_candidates.append(candidate)\n",
    "            ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "            sequences = ordered[:beamsize]\n",
    "            if exceed:\n",
    "                break\n",
    "\n",
    "        for i in sequences:\n",
    "            decoded_results.append(processor.decode(i[0]))\n",
    "\n",
    "    return decoded_results\n",
    "\n",
    "def predict(data, GPT_FIX=False):\n",
    "    features = processor(data[\"speech\"], sampling_rate=data[\"sampling_rate\"], padding=True, return_tensors=\"pt\")\n",
    "    input_values = features.input_values.to(device)\n",
    "    attention_mask = features.attention_mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values, attention_mask=attention_mask).logits\n",
    "    \n",
    "    decoded_results = []\n",
    "    for logit in logits:\n",
    "        pred_ids = torch.argmax(logit, dim=-1)\n",
    "        mask = pred_ids.ge(1).unsqueeze(-1).expand(logit.size())\n",
    "        vocab_size = logit.size()[-1]\n",
    "        voice_prob = torch.nn.functional.softmax((torch.masked_select(logit, mask).view(-1,vocab_size)),dim=-1)\n",
    "        gpt_input = torch.cat((torch.tensor([tokenizer.cls_token_id]).to(device),pred_ids[pred_ids>0]), 0)\n",
    "        gpt_prob = torch.nn.functional.softmax(lm_model(gpt_input).logits, dim=-1)[:voice_prob.size()[0],:]\n",
    "        if GPT_FIX: comb_pred_ids = torch.argmax(gpt_prob*voice_prob, dim=-1)\n",
    "        else: comb_pred_ids = torch.argmax(voice_prob, dim=-1)\n",
    "        decoded_results.append(processor.decode(comb_pred_ids))\n",
    "\n",
    "    return decoded_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdata = load_file_to_data('xxxx.wav')\n",
    "x = ipd.Audio(data=vdata['speech'], autoplay=False, rate=16000)\n",
    "print(predict(vdata))\n",
    "ipd.display(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}